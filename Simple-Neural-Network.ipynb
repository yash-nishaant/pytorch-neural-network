{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a simple Neural Network in Pytorch\n",
    "\n",
    "The code below seeks to implement a basic neural network, optimized for hyperparamter testing and core performance, using the Pytorch framework. The program uses the F-MNIST dataset, available through the torchvision library.\n",
    "\n",
    "Cells <a id ='imports'>[1]</a> and <a id ='version'>[2]</a> contain necessary imports, and version information for torch and torchvision respectively.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "torch.set_printoptions(linewidth=120)\n",
    "torch.set_grad_enabled(True)\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from collections import OrderedDict\n",
    "from collections import namedtuple\n",
    "from itertools import product\n",
    "from IPython import display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.0\n",
      "0.5.0\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network\n",
    "\n",
    "The neural network is a simple five-layer neural network with two convolution layers and three linear layers, the last of which is the ouput layer.\n",
    "\n",
    "The Network class extends the <code>nn.Module</code> class. This class keeps track of all the weights in the layers and allows network to update towards gaining higher accuracy. It is composed on an <code>\\__init__</code> constructor and a <code>forward</code> function. The <code>super()</code> function allows us to inherit functionality from the Network class to subclasses.\n",
    "\n",
    "The two convolutional networks perform a convolution operation. <code>in_channels</code> are data dependent hyperparameters, initially on the color channel information of the image and then on based on the number of <code>out_channels</code> obtained from the previous layer. The <code>kernel_size</code> and <code>out_channels</code> are manually set hyperparameters. \n",
    "\n",
    "Note that when passing from convolutional layer to linear layer, the data is flattened (hence the $12*4*4$, where 12 is the number of output channels from the previous layer and $4*4$ are the dimensions of the image tensor. The <code>out_features</code> are hyperparameters, whereas <code>in_features</code> depend on the number of output features of the previous layer. The output layer <code>out_features</code> correspond with the number of label classes in the data. \n",
    "\n",
    "The <code>forward</code> method describes the propagation of the data through the network. The convolution layers take a tensor, run it through a <b>convolution</b> operation, followed by a <b>relu activation</b> operation and a <b>max-pooling</b> operation. The linear layers use a <b>relu activation </b>. The first layer reshapes the data passing from a convolutional to a linear layer before applying the activation function. The output layer typically uses <b>softmax</b> function to return probability for a single-category label class. However, softmax isn't used here as the loss function used later in the implementation performs an implicit softmax.       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=12*4*4, out_features=120)\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=60)\n",
    "        self.out = nn.Linear(in_features=60, out_features=10)\n",
    "    \n",
    "    def forward(self, t):\n",
    "        #layer (1)\n",
    "        t = F.relu(self.conv1(t))\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "        \n",
    "        #Layer (2)\n",
    "        t = F.relu(self.conv2(t))\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "        \n",
    "        #Layer (3)\n",
    "        t = F.relu(self.fc1(t.reshape(-1, 12*4*4)))\n",
    "        \n",
    "        #Layer(4)\n",
    "        t = F.relu(self.fc2(t))\n",
    "        \n",
    "        #Layer(5)\n",
    "        t = self.out(t)\n",
    "        \n",
    "        return t "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "The training dataset is initialized using torchvision utilities. The F-MNIST dataset is a dataset comprised of German retailer Zalando's article images consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a $28*28$ grayscale image, associated with a label from 1 of 10 classes. Zalando's intention was to create a direct drop-in replacment of the original MNIST dataset for benchmarking machine learning algorithms.\n",
    "\n",
    "Link to F-MNIST github: https://github.com/zalandoresearch/fashion-mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = torchvision.datasets.FashionMNIST(\n",
    "    root='./data/FashionMNIST'\n",
    "    ,train=True\n",
    "    ,download=True\n",
    "    ,transform=transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RunBuilder\n",
    "The <b>RunBuilder</b> class builds sets of parameters that define out runs. The class contains a static method <code>get_runs(params)</code>, which retreives the runs that the class builds based on the parameters. \n",
    "\n",
    "Some basic termniology: \n",
    "\n",
    "An <b>epoch</b> is a hyperparameter, which denotes the period in which an entire dataset is passed both forward and backward through the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunBuilder():\n",
    "    @staticmethod\n",
    "    def get_runs(params):\n",
    "        \n",
    "        Run = namedtuple('Run', params.keys()) #create tuple subclass which encapsulates data for each run\n",
    "        \n",
    "        runs = [] #list of runs\n",
    "        for v in product(*params.values()): #creates Cartesian product using parameter values\n",
    "            runs.append(Run(*v)) #appends obtained set of ordered pairs, which define each run, to runs list \n",
    "        \n",
    "        return runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RunManager\n",
    "The <b>RunManager</b> class builds the training loop and manages each of our runs (as instantiated by the RunBuilder class) inside the loop. The <code>\\__init__ </code> class contructor initializes the attributes we will need to keep track of data across epochs, including the number of epochs, running loss, correct predictions and start time.\n",
    "\n",
    "The <code>begin_epoch</code> and <code>end_epoch</code> methods allow us to manage these values across the epoch lifetime. The same is true for <code>begin_run</code> and <code>end_run</code> across a run.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunManager():\n",
    "    \n",
    "    def __init__(self):\n",
    "\n",
    "        self.epoch_count = 0 \n",
    "        self.epoch_loss = 0\n",
    "        self.epoch_num_correct = 0\n",
    "        self.epoch_start_time = None\n",
    "\n",
    "        self.run_params = None\n",
    "        self.run_count = 0\n",
    "        self.run_data = []\n",
    "        self.run_start_time = None\n",
    "\n",
    "        self.network = None #network initialization\n",
    "        self.loader = None #loader initialization\n",
    "        self.tb = None #tensorboard initialization\n",
    "    \n",
    "    def begin_run(self, run, network, loader): #start a run\n",
    "\n",
    "        self.run_start_time = time.time() #Capture run start time\n",
    "\n",
    "        self.run_params = run #pass in run parameters\n",
    "        self.run_count += 1 #increment run count\n",
    "\n",
    "        self.network = network #save network to run \n",
    "        self.loader = loader #save data loader\n",
    "        self.tb = SummaryWriter(comment=f'-{run}') #allows to uniquely identify run in Tensorboard (tb)\n",
    "\n",
    "        images, labels = next(iter(self.loader)) \n",
    "        grid = torchvision.utils.make_grid(images)\n",
    "\n",
    "        self.tb.add_image('images', grid) #add images to tb\n",
    "        self.tb.add_graph(self.network, images) #add network to tb\n",
    "    \n",
    "    def end_run(self): #end a run\n",
    "        \n",
    "        self.tb.close() #close tb run\n",
    "        self.epoch_count = 0 #reset epoch count back to zero\n",
    "    \n",
    "    def begin_epoch(self): #start an epoch\n",
    "        \n",
    "        self.epoch_start_time = time.time() #instantiate epoch start time\n",
    "\n",
    "        self.epoch_count += 1 #increment epoch number \n",
    "        self.epoch_loss = 0 #set running loss to zero\n",
    "        self.epoch_num_correct = 0 #set number of correct predictions to zero\n",
    "    \n",
    "    def end_epoch(self): #end an epoch\n",
    "\n",
    "        epoch_duration = time.time() - self.epoch_start_time #estimate epoch duration\n",
    "        run_duration = time.time() - self.run_start_time #estimate run duration\n",
    "\n",
    "        loss = self.epoch_loss / len(self.loader.dataset) #estimate loss w.r.t number of items in dataset \n",
    "        accuracy = self.epoch_num_correct / len(self.loader.dataset) #estimate correct predictions w.r.t number of items in dataset \n",
    "\n",
    "        self.tb.add_scalar('Loss', loss, self.epoch_count)\n",
    "        self.tb.add_scalar('Accuracy', accuracy, self.epoch_count)\n",
    "\n",
    "        for name, param in self.network.named_parameters(): #we iterate over and pass network parameters to tensorboard\n",
    "            self.tb.add_histogram(name, param, self.epoch_count)\n",
    "            self.tb.add_histogram(f'{name}.grad', param.grad, self.epoch_count)\n",
    "        \n",
    "        results = OrderedDict() #an dictionary 'results' is built, which contains all values we want to track and display\n",
    "        results[\"run\"] = self.run_count\n",
    "        results[\"epoch\"] = self.epoch_count\n",
    "        results[\"loss\"] = loss\n",
    "        results[\"accuracy\"] = accuracy\n",
    "        results[\"epoch duration\"] = epoch_duration\n",
    "        results[\"run duration\"] = run_duration\n",
    "        \n",
    "        for k,v in self.run_params._asdict().items(): #iterate over key-value pairs in parameters and add values to results dictionary \n",
    "            results[k] = v\n",
    "        self.run_data.append(results) #append results to run_data list \n",
    "        df = pd.DataFrame.from_dict(self.run_data, orient='columns') #convert list to pandas dataframe to obtain formatted output\n",
    "        \n",
    "        display.clear_output(wait=True) #clear current output\n",
    "        display.display(df) #display new data frame\n",
    "        \n",
    "    def track_loss(self, loss): #function to track loss across an epoch\n",
    "        self.epoch_loss += loss.item() * self.loader.batch_size #calculate loss for each item in batch\n",
    "\n",
    "    def track_num_correct(self, preds, labels): #calculate correct productions\n",
    "        self.epoch_num_correct += self._get_num_correct(preds, labels) #calculate predictions for each item in batch\n",
    "    \n",
    "    @torch.no_grad() #remove gradient counting for pytorch\n",
    "    def _get_num_correct(self, preds, labels): #private function to obtain number of correct predictions\n",
    "        return preds.argmax(dim=1).eq(labels).sum().item() #returns number of correct predictions\n",
    "    \n",
    "    def save(self, fileName): #save results to csv and json files \n",
    "        \n",
    "        pd.DataFrame.from_dict(\n",
    "            self.run_data\n",
    "            ,orient='columns'\n",
    "        ).to_csv(f'{fileName}.csv')\n",
    "        \n",
    "        with open(f'{fileName}.json', 'w', encoding = 'utf-8') as f:\n",
    "            json.dump(self.run_data, f, ensure_ascii=False, indent=4) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then set of parameters we would like to vary across runs, including the <b>loss rate</b> <code>lr</code> and <b>batch size</b>. The <code>num_workers</code> parameter is for performance optimization, and comes from the <code>torch.utils.data.DataLoader</code> class, which allows to designate the number of subprocesses being used for each run, allowing to harness multi-core CPUs for parallelized tasking and reducing training time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run</th>\n",
       "      <th>epoch</th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>epoch duration</th>\n",
       "      <th>run duration</th>\n",
       "      <th>lr</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>num_workers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.584497</td>\n",
       "      <td>0.779967</td>\n",
       "      <td>24.063153</td>\n",
       "      <td>24.263617</td>\n",
       "      <td>0.01</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.565245</td>\n",
       "      <td>0.789283</td>\n",
       "      <td>17.056375</td>\n",
       "      <td>17.920956</td>\n",
       "      <td>0.01</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.587211</td>\n",
       "      <td>0.778050</td>\n",
       "      <td>17.499190</td>\n",
       "      <td>18.358734</td>\n",
       "      <td>0.01</td>\n",
       "      <td>100</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.566234</td>\n",
       "      <td>0.789517</td>\n",
       "      <td>17.562022</td>\n",
       "      <td>18.588276</td>\n",
       "      <td>0.01</td>\n",
       "      <td>100</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.588382</td>\n",
       "      <td>0.773350</td>\n",
       "      <td>17.729574</td>\n",
       "      <td>19.316330</td>\n",
       "      <td>0.01</td>\n",
       "      <td>100</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.603205</td>\n",
       "      <td>0.772783</td>\n",
       "      <td>19.035080</td>\n",
       "      <td>21.723888</td>\n",
       "      <td>0.01</td>\n",
       "      <td>100</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1.022862</td>\n",
       "      <td>0.608183</td>\n",
       "      <td>17.955968</td>\n",
       "      <td>19.001172</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1.030616</td>\n",
       "      <td>0.601267</td>\n",
       "      <td>12.346971</td>\n",
       "      <td>13.905802</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.976413</td>\n",
       "      <td>0.629183</td>\n",
       "      <td>12.927420</td>\n",
       "      <td>14.594959</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1.020133</td>\n",
       "      <td>0.616567</td>\n",
       "      <td>12.923430</td>\n",
       "      <td>14.841300</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>1.046391</td>\n",
       "      <td>0.604383</td>\n",
       "      <td>13.207669</td>\n",
       "      <td>15.486574</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1.050654</td>\n",
       "      <td>0.609317</td>\n",
       "      <td>14.626874</td>\n",
       "      <td>18.057052</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>2.267565</td>\n",
       "      <td>0.221700</td>\n",
       "      <td>13.412122</td>\n",
       "      <td>20.480215</td>\n",
       "      <td>0.01</td>\n",
       "      <td>10000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>2.158881</td>\n",
       "      <td>0.216633</td>\n",
       "      <td>10.306430</td>\n",
       "      <td>18.289077</td>\n",
       "      <td>0.01</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>2.185145</td>\n",
       "      <td>0.188333</td>\n",
       "      <td>10.044134</td>\n",
       "      <td>18.285089</td>\n",
       "      <td>0.01</td>\n",
       "      <td>10000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>2.065889</td>\n",
       "      <td>0.244583</td>\n",
       "      <td>10.894857</td>\n",
       "      <td>19.767122</td>\n",
       "      <td>0.01</td>\n",
       "      <td>10000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>2.182300</td>\n",
       "      <td>0.186067</td>\n",
       "      <td>11.724407</td>\n",
       "      <td>20.716355</td>\n",
       "      <td>0.01</td>\n",
       "      <td>10000</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>2.211425</td>\n",
       "      <td>0.187283</td>\n",
       "      <td>13.400346</td>\n",
       "      <td>24.195470</td>\n",
       "      <td>0.01</td>\n",
       "      <td>10000</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    run  epoch      loss  accuracy  epoch duration  run duration    lr  \\\n",
       "0     1      1  0.584497  0.779967       24.063153     24.263617  0.01   \n",
       "1     2      1  0.565245  0.789283       17.056375     17.920956  0.01   \n",
       "2     3      1  0.587211  0.778050       17.499190     18.358734  0.01   \n",
       "3     4      1  0.566234  0.789517       17.562022     18.588276  0.01   \n",
       "4     5      1  0.588382  0.773350       17.729574     19.316330  0.01   \n",
       "5     6      1  0.603205  0.772783       19.035080     21.723888  0.01   \n",
       "6     7      1  1.022862  0.608183       17.955968     19.001172  0.01   \n",
       "7     8      1  1.030616  0.601267       12.346971     13.905802  0.01   \n",
       "8     9      1  0.976413  0.629183       12.927420     14.594959  0.01   \n",
       "9    10      1  1.020133  0.616567       12.923430     14.841300  0.01   \n",
       "10   11      1  1.046391  0.604383       13.207669     15.486574  0.01   \n",
       "11   12      1  1.050654  0.609317       14.626874     18.057052  0.01   \n",
       "12   13      1  2.267565  0.221700       13.412122     20.480215  0.01   \n",
       "13   14      1  2.158881  0.216633       10.306430     18.289077  0.01   \n",
       "14   15      1  2.185145  0.188333       10.044134     18.285089  0.01   \n",
       "15   16      1  2.065889  0.244583       10.894857     19.767122  0.01   \n",
       "16   17      1  2.182300  0.186067       11.724407     20.716355  0.01   \n",
       "17   18      1  2.211425  0.187283       13.400346     24.195470  0.01   \n",
       "\n",
       "    batch_size  num_workers  \n",
       "0          100            0  \n",
       "1          100            1  \n",
       "2          100            2  \n",
       "3          100            4  \n",
       "4          100            8  \n",
       "5          100           12  \n",
       "6         1000            0  \n",
       "7         1000            1  \n",
       "8         1000            2  \n",
       "9         1000            4  \n",
       "10        1000            8  \n",
       "11        1000           12  \n",
       "12       10000            0  \n",
       "13       10000            1  \n",
       "14       10000            2  \n",
       "15       10000            4  \n",
       "16       10000            8  \n",
       "17       10000           12  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "params = OrderedDict(\n",
    "    lr = [.01]\n",
    "    ,batch_size = [100, 1000, 10000]\n",
    "    ,num_workers = [0, 1, 2, 4, 8, 12]\n",
    ")\n",
    "\n",
    "m = RunManager() \n",
    "for run in RunBuilder.get_runs(params): \n",
    "    \n",
    "    network = Network()\n",
    "    loader = torch.utils.data.DataLoader(train_set, batch_size=run.batch_size, num_workers=run.num_workers) #data loader\n",
    "    optimizer = optim.Adam(network.parameters(), lr=run.lr) #using Adam optimizer\n",
    "\n",
    "    m.begin_run(run, network, loader) #run begins\n",
    "\n",
    "    for epoch in range(1):\n",
    "        m.begin_epoch()\n",
    "        for batch in loader:\n",
    "            images, labels = batch \n",
    "            preds = network(images) \n",
    "            loss = F.cross_entropy(preds, labels) #cross_entropy loss function performs softmax intuitively  \n",
    "            optimizer.zero_grad() #prevents gradient accumulation for every occurance of backpropagation \n",
    "            loss.backward() #calculates derivative loss w.r.t x for every parameter x \n",
    "            optimizer.step() #updates weight tensors of network\n",
    "\n",
    "            m.track_loss(loss) \n",
    "            m.track_num_correct(preds, labels)\n",
    "        \n",
    "        m.end_epoch()\n",
    "    m.end_run()\n",
    "m.save('results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
